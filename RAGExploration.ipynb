{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "533887a6-7288-4e27-ae8e-ead8dc136b93",
   "metadata": {},
   "source": [
    "Referenced from [Langchain](https://python.langchain.com/docs/tutorials/rag/); this was enough of a RAG to figure out where the pain points are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f93e663-b35e-4fcc-8c8d-3d9ad5280aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade langchain langchain-community langchain-chroma\n",
    "%pip install --quiet --upgrade pypdf #TODO PyPDF prob doesn't have native code/is slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5623b1-518c-4ee3-a1e2-9a4019d00ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "# TODO: There's a bunch of table reqs in this format; probably screws stuff up.\n",
    "loader = PyPDFLoader(\"req_docs/20190029153.pdf\") # GATEWAY SYSTEM REQUIREMENTS\n",
    "docs = loader.load_and_split() # Split based on page breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a149b4-799e-4c1e-ae89-21b7cd09fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See, the tables screw things up.\n",
    "print(docs[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9e64e1-c8ce-4203-b9af-af74fbb92eba",
   "metadata": {},
   "source": [
    "Document too large, split it into chunks with overlap.\n",
    "\n",
    "> We use the `RecursiveCharacterTextSplitter`, which will recursively split the document using common separators like new lines until each chunk is the appropriate size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696dda9-7125-4844-b968-6c8ae37832bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "CHUNK_SIZE=1000\n",
    "CHUNK_OVERLAP=200\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, add_start_index=True\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4d6d71-865c-4f02-a6e8-323edf6bc9b1",
   "metadata": {},
   "source": [
    "It seems there are `splitters` oriented toward specific types of documents: there are loaders for \"source code\" and \"academic dissertations\".\n",
    "\n",
    "Perhaps write something for Requirements Documents... ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb3b37-ffcd-4245-a491-969c79bf6899",
   "metadata": {},
   "source": [
    "[MBET Embedding Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cfc9cc-b206-4783-ac0f-fe0a0cfacfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --quiet --upgrade langchain_voyageai\n",
    "# %pip install --quiet --upgrade langchain_openai\n",
    "# %pip install -qU langchain_ollama\n",
    "%pip install -qU langchain-huggingface\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# from langchain_voyageai import VoyageAIEmbeddings\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# embeddings = VoyageAIEmbeddings(\n",
    "#    voyage_api_key=\"paChL4fTAr1Rbb9YcJ1Rmo\", model=\"voyage-2\"\n",
    "# )\n",
    "\n",
    "# local_embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "# print(local_embeddings)\n",
    "# TODO: I've either got to pay $$$ for embeddings for a provider, or fetch a computer that can run ollama! :-)\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(embeddings_model)\n",
    "\n",
    "# I'm going to slice the splits to just 1 at this point for PoC.\n",
    "# vectorstore = Chroma.from_documents(documents=all_splits[100], embedding=embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca945d59-0a6c-47bd-8469-20b55a617759",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43ccf05-ec14-4f03-be1b-9fbefde079bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Active and Passive thermal systems should be designed with enough robustness and capacity to allow Gateway to fly in attitudes that are driven/support operations for extended periods.\"\n",
    "sim = vectorstore.similarity_search(question)\n",
    "print(len(sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e9ea43-be02-43de-bdaa-a201a9aaad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sim[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e90cba-9416-40e9-b860-6b9f68d71724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# TODO: Integrate the prompts.. what are we supposed to ask here?\n",
    "# TODO: Where's my inference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd02784-836d-434c-a4e9-de559a69cb17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
